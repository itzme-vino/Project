# Importing the correct module from google.protobuf
from google.protobuf.internal import encoder
import torch
import os
import tensorflow.compat.v1 as tf
import magenta.music as mm
import google.protobuf
from magenta.models.music_vae import TrainedModel
from magenta.music.sequences_lib import concatenate_sequences
from transformers import DistilBertTokenizer, DistilBertForSequenceClassification

# Define base directory where genre folders are located
base_directory = 'C:/Users/rajvi/OneDrive/Desktop/Datasets-Music/'

# Define genre folders
genre_folders = {
    'pop': 'Pop_Music_Midi',
    #'rock': 'rock_folder',
    'jazz': 'Jazz Midi',
    'hip_hop': 'Hip_Hop_MIDI',
   # 'folk': 'folk_folder',
    #'metal': 'metal_folder',
    'classical': 'classical_MIDI',
}

# Initialize an empty dictionary to store dataset paths by genre
dataset_path = {}

# Loop through each genre folder and collect MIDI file paths
for genre, folder_name in genre_folders.items():
    folder_path = os.path.join(base_directory, folder_name)
    midi_files = [os.path.join(folder_path, file) for file in os.listdir(folder_path) if file.endswith('.mid')]
    dataset_path[genre] = midi_files

# Define genre mappings for each mood
mood_genre_mapping = {
    'happy': 'pop',
    'joy': 'pop',
    'love': 'pop',
    'excited': 'pop',  # You can change this if you want
    'sad': 'classical',
    'depressed': 'hip hop',
    'angry': 'metal',
    'fear': 'metal',  
    'anxious': 'jazz'  
}

# Define your MusicVAE model configuration
music_vae_config = {
    "note_seq": {
        "converter": "melody",
        "drum_pitches": [],
        "extract_drum_tracks": False,
        "steps_per_quarter": 4
    },
    "hparams": {
        "batch_size": 64,
        "latent_size": 512,
        "learning_rate": 0.01,
        "dropout_keep_prob": 0.5,
        "grad_clip": 1.0,
        "max_seq_len": 64,
        "free_bits": 0,
        "max_beta": 0.2,
        "beta_rate": 0.9999,
        "gen_batch_size": 1,
        "clip_mode": "global_norm"
    }
}

# Initialize a MusicVAE model
music_vae = TrainedModel(music_vae_config, batch_size=4, checkpoint_dir_or_path=None)

# Load and preprocess the dataset
dataset = []
for mood, paths in dataset_path.items():
    sequences = [mm.midi_io.midi_file_to_note_sequence(path) for path in paths]
    dataset.extend([ConditionalEventSequence.from_note_sequence(sequence, mood) for sequence in sequences])

# Train the model on the custom dataset
music_vae.train(dataset, num_steps=20000)

# Save the trained model
music_vae.save("trained_music_vae_model")

# Load the pre-trained DistilBERT tokenizer and model for emotion detection
tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')
model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased-emotion')

# Function to detect mood from text using DistilBERT
def detect_mood_from_text(input_text):
    inputs = tokenizer(input_text, return_tensors="pt", truncation=True, padding=True)
    outputs = model(**inputs)
    predicted_class = torch.argmax(outputs.logits).item()
    emotions = ['anger', 'joy', 'love', 'sadness', 'fear', 'surprise']
    return emotions[predicted_class]

# Prompt the user for input
user_input = input("Enter your mood or emotion: ")

# Detect the mood from the user input using DistilBERT
detected_mood = detect_mood_from_text(user_input)

# Map the detected mood to a genre
mapped_genre = mood_genre_mapping.get(detected_mood, 'pop')  # Default to 'pop' if mood not found

# Generate music with the detected mood and mapped genre
generated_sequence = music_vae.sample(n=1, length=64, condition=mapped_genre)

# Save the generated music to a MIDI file
mm.midi_io.note_sequence_to_midi_file(generated_sequence, f"generated_music_{mapped_genre}_{detected_mood}.mid")
